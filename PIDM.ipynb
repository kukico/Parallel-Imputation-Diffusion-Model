{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b985b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch,math,joblib, os\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "sys.path.append('codes/Python/mimic-iii/imputation/diffusion/LSTS/Github')\n",
    "\n",
    "from utils import load_data,create_sequences,ICPDataset\n",
    "from util import calc_diffusion_step_embedding\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from util import calc_diffusion_hyperparams\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "# from vae import VAE1D\n",
    "from typing import Tuple\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from deap import base, creator, tools, algorithms\n",
    "from fastdtw import fastdtw\n",
    "import random\n",
    "import multiprocessing\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4134112",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3be802f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data, test_data = load_data(time_minutes=250, look_back_minutes=25,full=1,remote=1)\n",
    "train_dataset = ICPDataset(create_sequences(train_data, num_prev_chunks=2, num_missing_chunks=12, num_next_chunks=2))\n",
    "val_dataset = ICPDataset(create_sequences(val_data, num_prev_chunks=2, num_missing_chunks=12, num_next_chunks=2))\n",
    "test_dataset = ICPDataset(create_sequences(test_data, num_prev_chunks=2, num_missing_chunks=12, num_next_chunks=2))\n",
    "# Create data loader\n",
    "batch_size = 32  # Adjust based on your hardware capabilities\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c9bd1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8970850070100894\n",
      "0.03812972535932084\n",
      "0.0647852676305898\n"
     ]
    }
   ],
   "source": [
    "# print percentage of length of train_data, val_data, test_data\n",
    "print(len(train_data)/len(train_data+val_data+test_data))\n",
    "print(len(val_data)/len(train_data+val_data+test_data))\n",
    "print(len(test_data)/len(train_data+val_data+test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621974e4",
   "metadata": {},
   "source": [
    "# Define Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92ab4ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def swish(x):\n",
    "    return x * torch.sigmoid(x)\n",
    "\n",
    "def frequency_loss(x_reconstructed, condition, weight=1.0):\n",
    "    \"\"\"\n",
    "    Compute frequency-domain loss between reconstructed and conditional waves.\n",
    "    Args:\n",
    "        x_reconstructed: Reconstructed signal x_0, shape [B, 1, L].\n",
    "        condition: Conditional wave (e.g., prev_context or next_context), shape [B, 1, L].\n",
    "        weight: Weight for frequency loss.\n",
    "    Returns:\n",
    "        Loss value.\n",
    "    \"\"\"\n",
    "    # Compute FFTs\n",
    "    recon_freq = torch.fft.rfft(x_reconstructed, dim=-1)\n",
    "    cond_freq = torch.fft.rfft(condition, dim=-1)\n",
    "\n",
    "    # Compute magnitude of frequencies\n",
    "    recon_mag = torch.abs(recon_freq)\n",
    "    cond_mag = torch.abs(cond_freq)\n",
    "\n",
    "    # L2 loss in frequency domain\n",
    "    freq_loss = F.mse_loss(recon_mag, cond_mag)\n",
    "\n",
    "    return weight * freq_loss\n",
    "    \n",
    "def reconstruct_signal(x_t, noise_pred, alpha_bar_t):\n",
    "    \"\"\"\n",
    "    Reconstruct x_0 from x_t and predicted noise.\n",
    "    Args:\n",
    "        x_t: Noisy input at time step t, shape [B, 1, L].\n",
    "        noise_pred: Predicted noise, shape [B, 1, L].\n",
    "        alpha_bar_t: Cumulative product of noise scales, scalar or tensor.\n",
    "    Returns:\n",
    "        Reconstructed signal x_0, shape [B, 1, L].\n",
    "    \"\"\"\n",
    "    return (x_t - (1 - alpha_bar_t).sqrt() * noise_pred) / alpha_bar_t.sqrt()\n",
    "\n",
    "def weighted_mse_loss(pred, target, weight, weights=1.0):\n",
    "    \"\"\"\n",
    "    Compute the weighted Mean Squared Error loss.\n",
    "\n",
    "    Args:\n",
    "        pred: Predicted segment [B, 1, L]\n",
    "        target: Ground truth segment [B, 1, L]\n",
    "        weight: Weight mask [B, 1, L]\n",
    "\n",
    "    Returns:\n",
    "        Weighted MSE loss.\n",
    "    \"\"\"\n",
    "    # Compute element-wise squared error\n",
    "    error = (pred - target) ** 2\n",
    "    # Apply the weight mask\n",
    "    weighted_error = error * weight\n",
    "    # Compute the mean of the weighted errors\n",
    "    loss = torch.mean(weighted_error)\n",
    "    return weights * loss\n",
    "\n",
    "def scale_signal(signal: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Scale the signal to have mean=0 and amplitude between -1 and +1.\n",
    "\n",
    "    Args:\n",
    "        signal (torch.Tensor): Input signal of shape (batch_size, 1, signal_length).\n",
    "\n",
    "    Returns:\n",
    "        scaled_signal (torch.Tensor): Scaled signal with mean=0 and amplitude [-1, 1].\n",
    "        original_mean (torch.Tensor): Original mean of the signal for each batch.\n",
    "        original_range (torch.Tensor): Original range (max - min) of the signal for each batch.\n",
    "    \"\"\"\n",
    "    original_mean = signal.mean(dim=2, keepdim=True)  # Compute mean along the signal length\n",
    "    centered_signal = signal - original_mean          # Subtract mean to center around 0\n",
    "    signal_min, _ = centered_signal.min(dim=2, keepdim=True)\n",
    "    signal_max, _ = centered_signal.max(dim=2, keepdim=True)\n",
    "    original_range = signal_max - signal_min          # Compute the original range\n",
    "    scaled_signal = centered_signal / (original_range / 2 + 1e-8)  # Scale to [-1, 1]\n",
    "\n",
    "    return scaled_signal, original_mean, original_range\n",
    "\n",
    "def unscale_signal(scaled_signal: torch.Tensor, original_mean: torch.Tensor, original_range: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Convert the scaled signal back to its original form.\n",
    "\n",
    "    Args:\n",
    "        scaled_signal (torch.Tensor): Scaled signal with mean=0 and amplitude [-1, 1].\n",
    "        original_mean (torch.Tensor): Original mean of the signal for each batch.\n",
    "        original_range (torch.Tensor): Original range (max - min) of the signal for each batch.\n",
    "\n",
    "    Returns:\n",
    "        original_signal (torch.Tensor): Signal scaled back to its original form.\n",
    "    \"\"\"\n",
    "    centered_signal = scaled_signal * (original_range / 2)\n",
    "    original_signal = centered_signal + original_mean\n",
    "    return original_signal\n",
    "\n",
    "\n",
    "def calc_batch_diffusion_step_embedding(diffusion_steps, embed_dims=(64, 64)):\n",
    "    \"\"\"\n",
    "    Embed each column of diffusion_steps into a specific dimensional space and concatenate the results.\n",
    "\n",
    "    Parameters:\n",
    "    diffusion_steps (torch.tensor, shape=(batch_size, 3)): \n",
    "                                Input diffusion steps for the batch.\n",
    "    embed_dims (tuple):         Embedding dimensions for each column (default=(25, 200, 25)).\n",
    "\n",
    "    Returns:\n",
    "    torch.tensor, shape=(batch_size, sum(embed_dims)): \n",
    "                    Concatenated embeddings for all columns.\n",
    "    \"\"\"\n",
    "    batch_size, num_columns = diffusion_steps.shape\n",
    "    assert len(embed_dims) == num_columns, \"Embed dimensions must match the number of columns in diffusion_steps\"\n",
    "\n",
    "    embeddings = []\n",
    "    for col_idx, embed_dim in enumerate(embed_dims):\n",
    "        # Select the column and create an embedding matrix\n",
    "        col_steps = diffusion_steps[:, col_idx].unsqueeze(-1)  # Shape: (batch_size, 1)\n",
    "        half_dim = embed_dim // 2\n",
    "        _embed = torch.exp(torch.arange(half_dim) * -(torch.log(torch.tensor(10000.0)) / (half_dim - 1)))  # Shape: (half_dim,)\n",
    "        \n",
    "        # Compute sinusoidal embeddings\n",
    "        col_embed = col_steps * _embed  # Shape: (batch_size, half_dim)\n",
    "        col_embed = torch.cat([torch.sin(col_embed), torch.cos(col_embed)], dim=-1)  # Shape: (batch_size, embed_dim)\n",
    "        \n",
    "        # Append the embedding for the current column\n",
    "        embeddings.append(col_embed)\n",
    "\n",
    "    # Concatenate embeddings along the last dimension\n",
    "    output = torch.cat(embeddings, dim=-1)  # Shape: (batch_size, sum(embed_dims))\n",
    "    \n",
    "    return output\n",
    "\n",
    "class Conv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, dilation=1, stride=1):\n",
    "        super(Conv, self).__init__()\n",
    "        self.padding = dilation * (kernel_size - 1) // 2\n",
    "        self.conv = nn.Conv1d(\n",
    "            in_channels, \n",
    "            out_channels, \n",
    "            kernel_size, \n",
    "            dilation=dilation, \n",
    "            stride=stride,\n",
    "            # padding=self.padding)\n",
    "            padding='same')\n",
    "        self.conv = nn.utils.weight_norm(self.conv)\n",
    "        nn.init.kaiming_normal_(self.conv.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class ZeroConv1d(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel):\n",
    "        super(ZeroConv1d, self).__init__()\n",
    "        self.conv = nn.Conv1d(\n",
    "            in_channel, \n",
    "            out_channel, \n",
    "            kernel_size=1, \n",
    "            padding=0)\n",
    "        nn.init.zeros_(self.conv.weight)\n",
    "        nn.init.zeros_(self.conv.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "915f5137",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual_block(nn.Module):\n",
    "    def __init__(self, in_channels, res_channels, skip_channels, dilation, diffusion_embed_dim_out):\n",
    "        super(Residual_block, self).__init__()\n",
    "        \n",
    "        self.res_channels = res_channels\n",
    "\n",
    "        # Diffusion step embedding\n",
    "        self.fc_t = nn.Linear(diffusion_embed_dim_out, res_channels)\n",
    "\n",
    "        # Dilated convolution\n",
    "        self.dilated_conv = Conv(res_channels, res_channels, kernel_size=3, dilation=dilation)\n",
    "\n",
    "        # Embeddings\n",
    "        self.chunk_number_embedding = nn.Embedding(12, 16)\n",
    "\n",
    "        self.cond_conv = Conv(4, res_channels, kernel_size=1)\n",
    "\n",
    "        # FiLM generator\n",
    "        self.film_generator = nn.Sequential(\n",
    "            nn.Conv1d(16, 2 * res_channels, kernel_size=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Conditioner projections\n",
    "        self.conditioner_projection_prev = nn.Conv1d(1, res_channels, kernel_size=1)\n",
    "        self.conditioner_projection_next = nn.Conv1d(1, res_channels, kernel_size=1)\n",
    "\n",
    "        # Output projections\n",
    "        self.output_projection = nn.Conv1d(res_channels, 2 * res_channels, kernel_size=1)\n",
    "        self.residual_projection = nn.Conv1d(res_channels, res_channels, kernel_size=1)\n",
    "        self.skip_projection = nn.Conv1d(res_channels, skip_channels, kernel_size=1)\n",
    "\n",
    "        \n",
    "    def forward(self, x, cond_ref, cond_ctl, diffusion_step_embed):\n",
    "        \n",
    "        B, C, L = x.shape\n",
    "        assert C == self.res_channels\n",
    "\n",
    "        # Diffusion step embedding\n",
    "        part_t = self.fc_t(diffusion_step_embed).view(B, self.res_channels, 1)\n",
    "        h = x + part_t\n",
    "        \n",
    "        cond_ctl = cond_ctl.long()\n",
    "        cond_patient = self.chunk_number_embedding(cond_ctl).unsqueeze(-1)  # [B, 16, 1]\n",
    "        film_params = self.film_generator(cond_patient)  # [B, 2 * res_channels, 1\n",
    "        gamma, beta = torch.chunk(film_params, 2, dim=1)\n",
    "        h = gamma * h + beta  # Apply FiLM\n",
    "        \n",
    "        prev_context_pc_z1, prev_context_pc_z2, next_context_pc_z1, next_context_pc_z2 = cond_ref\n",
    "        \n",
    "        cond = torch.stack([\n",
    "            prev_context_pc_z1,\n",
    "            prev_context_pc_z2,\n",
    "            next_context_pc_z1,\n",
    "            next_context_pc_z2\n",
    "        ], dim=1)\n",
    "\n",
    "        cond = swish(self.cond_conv(cond))\n",
    "\n",
    "        # Combine the processed conditional inputs\n",
    "        h = h + cond # [B, res_channels, T]\n",
    "\n",
    "        # Continue with dilated convolution and gating\n",
    "        h = self.dilated_conv(h)\n",
    "        h = swish(h)\n",
    "        h = self.output_projection(h)\n",
    "        gate, filter = torch.chunk(h, 2, dim=1)\n",
    "        h = torch.sigmoid(gate) * torch.tanh(filter)\n",
    "        \n",
    "        # Residual and skip connections\n",
    "        residual = self.residual_projection(h)\n",
    "        skip = self.skip_projection(h)\n",
    "        \n",
    "        return (x + residual) * math.sqrt(0.5), skip\n",
    "          \n",
    "\n",
    "class Residual_group(nn.Module):\n",
    "    def __init__(self, in_channels, res_channels, skip_channels, num_res_layers, dilation_cycle, diffusion_embed_dim_in, diffusion_embed_dim_mid, diffusion_embed_dim_out):\n",
    "        super(Residual_group, self).__init__()\n",
    "\n",
    "        self.num_res_layers = num_res_layers\n",
    "\n",
    "        # Diffusion embedding layers\n",
    "        self.fc_t1 = nn.Linear(diffusion_embed_dim_in, diffusion_embed_dim_mid)\n",
    "        self.fc_t2 = nn.Linear(diffusion_embed_dim_mid, diffusion_embed_dim_out)\n",
    "\n",
    "        # Residual blocks\n",
    "        self.residual_blocks = nn.ModuleList([\n",
    "            Residual_block(\n",
    "                in_channels,\n",
    "                res_channels,\n",
    "                skip_channels,\n",
    "                dilation=2 ** (n % dilation_cycle),\n",
    "                diffusion_embed_dim_out=diffusion_embed_dim_out\n",
    "            ) for n in range(num_res_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x, cond_ref, cond_ctl, diffusion_steps):\n",
    "        # Ensure diffusion_steps is float\n",
    "        diffusion_steps = diffusion_steps.float()\n",
    "\n",
    "        # Compute diffusion step embeddings\n",
    "        diffusion_step_embed = calc_diffusion_step_embedding(diffusion_steps, self.fc_t1.in_features)\n",
    "        diffusion_step_embed = swish(self.fc_t1(diffusion_step_embed))\n",
    "        diffusion_step_embed = swish(self.fc_t2(diffusion_step_embed))\n",
    "\n",
    "        # Pass through residual blocks\n",
    "        skip_connections = 0\n",
    "        for res_block in self.residual_blocks:\n",
    "            x, skip = res_block(x, cond_ref, cond_ctl, diffusion_step_embed)\n",
    "            skip_connections += skip\n",
    "\n",
    "        return skip_connections * math.sqrt(1.0 / self.num_res_layers)\n",
    "\n",
    "class DiffWaveImputer(nn.Module):\n",
    "    def __init__(self, in_channels, res_channels, skip_channels, out_channels, num_res_layers, dilation_cycle, diffusion_embed_dim_in, diffusion_embed_dim_mid, diffusion_embed_dim_out):\n",
    "        super(DiffWaveImputer, self).__init__()\n",
    "\n",
    "        self.init_conv = nn.Sequential(\n",
    "            Conv(in_channels, res_channels, kernel_size=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.residual_layer = Residual_group(\n",
    "            in_channels=in_channels,\n",
    "            res_channels=res_channels,\n",
    "            skip_channels=skip_channels,\n",
    "            num_res_layers=num_res_layers,\n",
    "            dilation_cycle=dilation_cycle,\n",
    "            diffusion_embed_dim_in=diffusion_embed_dim_in,\n",
    "            diffusion_embed_dim_mid=diffusion_embed_dim_mid,\n",
    "            diffusion_embed_dim_out=diffusion_embed_dim_out\n",
    "        )\n",
    "\n",
    "        self.final_conv = nn.Sequential(\n",
    "            Conv(skip_channels, skip_channels, kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            ZeroConv1d(skip_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, noise, cond_ref, cond_ctl, diffusion_steps):\n",
    "        x = self.init_conv(noise)\n",
    "        x = self.residual_layer(x, cond_ref, cond_ctl, diffusion_steps)\n",
    "        return self.final_conv(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e63bf6a",
   "metadata": {},
   "source": [
    "# Define Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869398d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "diffwave = DiffWaveImputer(\n",
    "    in_channels = 1,\n",
    "    out_channels = 1,\n",
    "    num_res_layers = 8,\n",
    "    res_channels = 64,\n",
    "    skip_channels = 64,\n",
    "    dilation_cycle = 7,\n",
    "    diffusion_embed_dim_in = 64,\n",
    "    diffusion_embed_dim_mid = 128,\n",
    "    diffusion_embed_dim_out = 128\n",
    ")\n",
    "diffwave = diffwave.cuda()\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "        print(f\"Let's use {torch.cuda.device_count()} GPUs!\")\n",
    "        diffwave = nn.DataParallel(diffwave)\n",
    "optimizer = torch.optim.Adam(diffwave.parameters(), lr=1e-3)#, weight_decay=1e-4)\n",
    "scheduler = ExponentialLR(optimizer, gamma=0.95)\n",
    "epochs = 50\n",
    "\n",
    "diffusion_config = {\n",
    "        \"T\": 200,\n",
    "        \"beta_0\": 0.0001,\n",
    "        \"beta_T\": 0.02\n",
    "    }  # basic hyperparameters\n",
    "diffusion_hyperparams = calc_diffusion_hyperparams(**diffusion_config)  # di8 ctionary of all diffusion hyperparameters\n",
    "diffusion_hyperparams = {k: torch.tensor(v).cuda() for k, v in diffusion_hyperparams.items()}  # convert to tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b950bd96",
   "metadata": {},
   "source": [
    "# Training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331a9fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_loss = 1000\n",
    "loss_fn = nn.MSELoss()\n",
    "overlap_length = 25\n",
    "\n",
    "for i in range(epochs):\n",
    "\n",
    "    diffwave.train()\n",
    "    train_loss = 0\n",
    "    \n",
    "    for prev_context_pc, next_context_pc, target_pc, patient_idx, chunk_number, start_pos, enc_pos in tqdm(train_dataloader):\n",
    "        \n",
    "        T, Alpha_bar = diffusion_hyperparams[\"T\"], diffusion_hyperparams[\"Alpha_bar\"]\n",
    "\n",
    "        chunk_number = chunk_number[0].to('cuda')\n",
    "        \n",
    "        target_pc_scaled_signal, target_pc_original_mean, target_pc_original_range = scale_signal(target_pc.unsqueeze(-1).permute(0,2,1).to('cuda'))     \n",
    "        prev_context_pc_1_scaled_signal, prev_context_pc_1_original_mean, prev_context_pc_1_original_range = scale_signal(prev_context_pc[:,0:250].unsqueeze(-1).permute(0,2,1).to('cuda'))\n",
    "        prev_context_pc_2_scaled_signal, prev_context_pc_2_original_mean, prev_context_pc_2_original_range = scale_signal(prev_context_pc[:,250:500].unsqueeze(-1).permute(0,2,1).to('cuda')) \n",
    "        next_context_pc_1_scaled_signal, next_context_pc_1_original_mean, next_context_pc_1_original_range = scale_signal(next_context_pc[:,0:250].unsqueeze(-1).permute(0,2,1).to('cuda')) \n",
    "        next_context_pc_2_scaled_signal, next_context_pc_2_original_mean, next_context_pc_2_original_range = scale_signal(next_context_pc[:,250:500].unsqueeze(-1).permute(0,2,1).to('cuda'))\n",
    "    \n",
    "        # target_pc_index =  torch.stack([\n",
    "        #                         226 * (chunk_number+2),\n",
    "        #                         226 * (chunk_number+2) + 226\n",
    "        #                     ], dim=1)\n",
    "\n",
    "        audio = target_pc_scaled_signal\n",
    "        cond_data = (prev_context_pc_1_scaled_signal.squeeze(1), prev_context_pc_2_scaled_signal.squeeze(1), next_context_pc_1_scaled_signal.squeeze(1), next_context_pc_2_scaled_signal.squeeze(1))\n",
    "\n",
    "        B, C, L = audio.shape  # B is batchsize, C=1, L is audio length\n",
    "        diffusion_steps = torch.randint(T, size=(target_pc.shape[0], 1, 1)).cuda()  # randomly sample diffusion steps from 1~T\n",
    "\n",
    "        z = torch.normal(0, 1, size=audio.shape).cuda()\n",
    "        transformed_X = torch.sqrt(Alpha_bar[diffusion_steps]) * audio + torch.sqrt(1 - Alpha_bar[diffusion_steps]) * z  # compute x_t from q(x_t|x_0)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        epsilon_theta = diffwave(transformed_X, cond_data, chunk_number, diffusion_steps.view(B, 1)) # predict \\epsilon according to \\epsilon_\\theta\n",
    "\n",
    "        # 1. noise loss\n",
    "        noise_loss = loss_fn(epsilon_theta,z)   \n",
    "        \n",
    "        total_loss = noise_loss # + mse_loss + freq_loss_value #+ 0.2*diff_loss\n",
    "        total_loss.backward()\n",
    "        train_loss += total_loss.item()\n",
    "        optimizer.step()\n",
    "        # print(train_loss)\n",
    "    scheduler.step()\n",
    "    diffwave.eval()\n",
    "    val_loss = 0\n",
    "    \n",
    "    for prev_context_pc, next_context_pc, target_pc, patient_idx, chunk_number, start_pos, enc_pos in tqdm(val_dataloader):\n",
    "\n",
    "        T, Alpha, Alpha_bar, Sigma = diffusion_hyperparams[\"T\"], diffusion_hyperparams[\"Alpha\"], diffusion_hyperparams[\"Alpha_bar\"], diffusion_hyperparams[\"Sigma\"]\n",
    "\n",
    "        chunk_number = chunk_number[0].to('cuda')\n",
    "        \n",
    "        target_pc_scaled_signal, target_pc_original_mean, target_pc_original_range = scale_signal(target_pc.unsqueeze(-1).permute(0,2,1).to('cuda'))\n",
    "        prev_context_pc_1_scaled_signal, prev_context_pc_1_original_mean, prev_context_pc_1_original_range = scale_signal(prev_context_pc[:,0:250].unsqueeze(-1).permute(0,2,1).to('cuda'))\n",
    "        prev_context_pc_2_scaled_signal, prev_context_pc_2_original_mean, prev_context_pc_2_original_range = scale_signal(prev_context_pc[:,250:500].unsqueeze(-1).permute(0,2,1).to('cuda')) \n",
    "        next_context_pc_1_scaled_signal, next_context_pc_1_original_mean, next_context_pc_1_original_range = scale_signal(next_context_pc[:,0:250].unsqueeze(-1).permute(0,2,1).to('cuda'))\n",
    "        next_context_pc_2_scaled_signal, next_context_pc_2_original_mean, next_context_pc_2_original_range = scale_signal(next_context_pc[:,250:500].unsqueeze(-1).permute(0,2,1).to('cuda'))\n",
    "\n",
    "        target_pc_index =  torch.stack([\n",
    "                                226 * (chunk_number+2),\n",
    "                                226 * (chunk_number+2) + 226\n",
    "                            ], dim=1)\n",
    "\n",
    "        # audio = target_pc_scaled_signal.unsqueeze(-1).permute(0,2,1)\n",
    "        audio = target_pc_scaled_signal\n",
    "        cond_data = (prev_context_pc_1_scaled_signal.squeeze(1), prev_context_pc_2_scaled_signal.squeeze(1), next_context_pc_1_scaled_signal.squeeze(1), next_context_pc_2_scaled_signal.squeeze(1))\n",
    "        \n",
    "        size = (audio.shape[0],1,audio.shape[2])\n",
    "        x = torch.normal(0, 1, size=size).cuda()\n",
    "        \n",
    "        B, C, L = audio.shape  # B is batchsize, C=1, L is audio length\n",
    "        \n",
    "        eta_initial = 1.0  # Full noise initially\n",
    "        eta_final = 0.1    # Minimal noise at the last step\n",
    "        T = diffusion_hyperparams[\"T\"]\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            for t in (range(T - 1, -1, -1)):\n",
    "                diffusion_steps = (t * torch.ones((size[0], 1))).cuda()\n",
    "                epsilon_theta = diffwave(x, cond_data, chunk_number, diffusion_steps.view(B, 1))\n",
    "                x = (x - (1 - Alpha[t]) / torch.sqrt(1 - Alpha_bar[t]) * epsilon_theta) / torch.sqrt(Alpha[t])\n",
    "                if t > 0:\n",
    "                    x = x + Sigma[t] * torch.normal(0, 1, size=size).cuda()  # add the variance term to x_{t-1}\n",
    "        break\n",
    "    \n",
    "    # reconstructed = vae.decode(x)\n",
    "    reconstructed_unscale = unscale_signal(x, target_pc_original_mean, target_pc_original_range)\n",
    "    \n",
    "    loss = loss_fn(reconstructed_unscale[0,0,0:250], target_pc.unsqueeze(-1).permute(0,2,1).to('cuda'))\n",
    "#     loss = loss_fn(x,target_pc_z)\n",
    "    val_loss += loss.item()\n",
    "\n",
    "    print(\"iteration: {} \\t train_loss: {:.4f} \\t val_loss: {:.4f} \".format(i, train_loss/len(train_dataloader), val_loss))\n",
    "    torch.save(diffwave.state_dict(), \"stable_diffusion_autodl.pt\")\n",
    "    # wandb.log({\"epoch\": i, \"train_loss\": train_loss/len(train_dataloader), \"val_loss\":val_loss})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac7d09e",
   "metadata": {},
   "source": [
    "# Use PIDM to impute segments on Evaluation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ff36b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5633/5633 [1:36:54<00:00,  1.03s/it]\n"
     ]
    }
   ],
   "source": [
    "data_ori = []\n",
    "data_imp = []\n",
    "data_pre = []\n",
    "data_next = []\n",
    "for prev_context_pc, next_context_pc, target_pc, patient_idx, chunk_number, start_pos, enc_pos in tqdm(val_dataloader):\n",
    "\n",
    "    T, Alpha, Alpha_bar, Sigma = diffusion_hyperparams[\"T\"], diffusion_hyperparams[\"Alpha\"], diffusion_hyperparams[\"Alpha_bar\"], diffusion_hyperparams[\"Sigma\"]\n",
    "\n",
    "    chunk_number = chunk_number[0].to('cuda')\n",
    "        \n",
    "    target_pc_scaled_signal, target_pc_original_mean, target_pc_original_range = scale_signal(target_pc.unsqueeze(-1).permute(0,2,1).to('cuda'))     \n",
    "    prev_context_pc_1_scaled_signal, prev_context_pc_1_original_mean, prev_context_pc_1_original_range = scale_signal(prev_context_pc[:,0:250].unsqueeze(-1).permute(0,2,1).to('cuda'))\n",
    "    prev_context_pc_2_scaled_signal, prev_context_pc_2_original_mean, prev_context_pc_2_original_range = scale_signal(prev_context_pc[:,250:500].unsqueeze(-1).permute(0,2,1).to('cuda')) \n",
    "    next_context_pc_1_scaled_signal, next_context_pc_1_original_mean, next_context_pc_1_original_range = scale_signal(next_context_pc[:,0:250].unsqueeze(-1).permute(0,2,1).to('cuda')) \n",
    "    next_context_pc_2_scaled_signal, next_context_pc_2_original_mean, next_context_pc_2_original_range = scale_signal(next_context_pc[:,250:500].unsqueeze(-1).permute(0,2,1).to('cuda'))\n",
    "    \n",
    "    # target_pc_index =  torch.stack([\n",
    "    #                         226 * (chunk_number+2),\n",
    "    #                         226 * (chunk_number+2) + 226\n",
    "    #                     ], dim=1)\n",
    "    \n",
    "    audio = target_pc_scaled_signal\n",
    "    cond_data = (prev_context_pc_1_scaled_signal.squeeze(1), prev_context_pc_2_scaled_signal.squeeze(1), next_context_pc_1_scaled_signal.squeeze(1), next_context_pc_2_scaled_signal.squeeze(1))\n",
    "    \n",
    "    B, C, L = audio.shape  # B is batchsize, C=1, L is audio length\n",
    "\n",
    "    size = (audio.shape[0],1,audio.shape[2])\n",
    "    x = torch.normal(0, 1, size=size).cuda()\n",
    "\n",
    "    B, C, L = audio.shape  # B is batchsize, C=1, L is audio length\n",
    "\n",
    "    eta_initial = 1.0  # Full noise initially\n",
    "    eta_final = 0.1    # Minimal noise at the last step\n",
    "    T = diffusion_hyperparams[\"T\"]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for t in (range(T - 1, -1, -1)):\n",
    "            diffusion_steps = (t * torch.ones((size[0], 1))).cuda()\n",
    "            epsilon_theta = diffwave(x, cond_data, chunk_number, diffusion_steps.view(B, 1))\n",
    "            x = (x - (1 - Alpha[t]) / torch.sqrt(1 - Alpha_bar[t]) * epsilon_theta) / torch.sqrt(Alpha[t])\n",
    "            if t > 0:\n",
    "                x = x + Sigma[t] * torch.normal(0, 1, size=size).cuda()  # add the variance term to x_{t-1}\n",
    "                \n",
    "    # reconstructed = vae.decode(x)\n",
    "    reconstructed_unscaled = unscale_signal(x,target_pc_original_mean, target_pc_original_range)\n",
    "\n",
    "    data_ori.append(target_pc.unsqueeze(-1).permute(0,2,1).to('cuda'))\n",
    "    data_imp.append(reconstructed_unscaled[:,:,:])\n",
    "    data_pre.append(prev_context_pc)\n",
    "    data_next.append(next_context_pc)\n",
    "\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e6e5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ori_t = data_ori\n",
    "data_imp_t = data_imp\n",
    "data_pre_t = data_pre\n",
    "data_next_t = data_next\n",
    "data_ori_ = torch.cat(data_ori_t, dim=0)\n",
    "data_imp_ = torch.cat(data_imp_t, dim=0)\n",
    "data_pre_ = torch.cat(data_pre_t, dim=0)\n",
    "data_next_ = torch.cat(data_next_t, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9891d481",
   "metadata": {},
   "source": [
    "# Use DPA to reconstruct the sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10637d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the original sequence length\n",
    "# data_ori_tensor = torch.cat(data_ori[0][0:252,:],dim=0).cpu().detach()\n",
    "# data_imp_tensor = torch.cat(data_imp[0][0:252,:],dim=0).cpu().detach()\n",
    "data_ori_tensor = data_ori_[:,0,:].cpu().detach()\n",
    "data_imp_tensor = data_imp_[:,0,:].cpu().detach()\n",
    "data_pre_tensor = data_pre_[:,:].cpu().detach()\n",
    "data_next_tensor = data_next_[:,:].cpu().detach()\n",
    "\n",
    "num_segments = 12\n",
    "segment_length = 250\n",
    "overlap = 25\n",
    "num_sequences = data_ori_tensor.size(0) // num_segments\n",
    "original_length = segment_length * (num_segments + 4) - overlap * (num_segments + 4 - 1)  # 2675\n",
    "\n",
    "# Reshape and reconstruct original sequence\n",
    "data_ori_reshaped = data_ori_tensor.view(num_sequences, num_segments, -1)\n",
    "data_pre_reshaped = data_pre_tensor.view(num_sequences, num_segments, -1)\n",
    "data_next_reshaped = data_next_tensor.view(num_sequences, num_segments, -1)\n",
    "\n",
    "# data_ori_reshaped is with shape (batch_size, num_segments, segment_length)\n",
    "# data_pre_reshaped is with shape (batch_size, num_segments, 2*segment_length)\n",
    "# data_next_reshaped is with shape (batch_size, num_segments, 2*segment_length)\n",
    "data_pre_reshape = data_pre_reshaped[:,0,:]\n",
    "# reshape the data_pre_reshaped to (batch_size,2, segment_length) \n",
    "data_pre_reshape = data_pre_reshape.view(num_sequences,2,-1)\n",
    "data_next_reshaped = data_next_reshaped[:,0,:]\n",
    "# reshape the data_next_reshaped to (batch_size,2, segment_length)\n",
    "data_next_reshape = data_next_reshaped.view(num_sequences,2,-1)\n",
    "\n",
    "# add data_ori_reshaped to the data_pre_reshape\n",
    "data_ori_reshape_cat = torch.cat((data_pre_reshape,data_ori_reshaped),dim=1)\n",
    "# add data_next_reshape to the data_ori_reshape_cat\n",
    "data_ori_reshape_cat = torch.cat((data_ori_reshape_cat,data_next_reshape),dim=1)\n",
    "\n",
    "reconstructed_ori = torch.zeros((num_sequences, original_length))\n",
    "# Reconstruct original sequences by considering overlaps\n",
    "\n",
    "for i in range(num_segments+4):\n",
    "    start = i * (segment_length - overlap)\n",
    "    end = start + segment_length\n",
    "    reconstructed_ori[:, start:end] += data_ori_reshape_cat[:, i, :].squeeze(-1)\n",
    "# Normalize overlap areas to avoid summing overlap contributions\n",
    "weights = torch.zeros(original_length)\n",
    "for i in range(num_segments+4):\n",
    "    start = i * (segment_length - overlap)\n",
    "    end = start + segment_length\n",
    "    weights[start:end] += 1\n",
    "weights = weights.unsqueeze(0)  # Broadcast across all sequences\n",
    "reconstructed_ori /= weights\n",
    "\n",
    "num_segments = 12\n",
    "segment_length = 250\n",
    "retain_length = 225  # Points to retain from each segment\n",
    "compare_length = 25  # Points to compare for the transition\n",
    "overlap = segment_length - retain_length\n",
    "\n",
    "# Reshape tensors for processing\n",
    "data_imp_reshaped = data_imp_tensor.view(-1, num_segments, segment_length)\n",
    "num_sequences = data_imp_reshaped.size(0)\n",
    "reconstructed_imp = torch.zeros((num_sequences, retain_length * num_segments))\n",
    "# Process `data_imp_tensor` to retain 226 points and optimize overlap transitions\n",
    "for seq in tqdm(range(num_sequences)):\n",
    "    last_point = None\n",
    "    start = 0\n",
    "    # print(\"seq\"+str(seq))\n",
    "    for i in range(num_segments):\n",
    "        segment = data_imp_reshaped[seq, i, :]\n",
    "        # print(i)\n",
    "        if i == 0:\n",
    "            last_point = data_pre_reshape[seq, 1, -1]\n",
    "            last_second_point = data_pre_reshape[seq, 1, -2]\n",
    "            last_derivatives = last_point-last_second_point\n",
    "            \n",
    "#             # Retain the first 226 points of the first segment\n",
    "#             reconstructed_imp[seq, start:start + retain_length] = segment[:retain_length]\n",
    "#         else:\n",
    "        compare_region = segment[:compare_length]\n",
    "        transition_abs_value = torch.abs(compare_region-last_point)\n",
    "        derivatives = torch.tensor(compare_region[1:] - compare_region[:-1])\n",
    "        if last_derivatives > 0:\n",
    "            positive_indices = (derivatives > 0).nonzero(as_tuple=True)[0]\n",
    "            if len(positive_indices > 0):\n",
    "                best_transition_point = transition_abs_value[positive_indices].argmin().item()\n",
    "                best_transition_point = positive_indices[best_transition_point].item()\n",
    "            else:\n",
    "                best_transition_point = transition_abs_value.argmin().item()\n",
    "        else:\n",
    "            negative_indices = (derivatives < 0).nonzero(as_tuple=True)[0]\n",
    "            if len(negative_indices > 0): \n",
    "                best_transition_point = transition_abs_value[negative_indices].argmin().item()\n",
    "                best_transition_point = negative_indices[best_transition_point].item()\n",
    "            else:\n",
    "                best_transition_point = transition_abs_value.argmin().item()\n",
    "\n",
    "        reconstructed_imp[seq, start:start + retain_length] = segment[best_transition_point+1:best_transition_point+1 + retain_length]\n",
    "#             if i == 2:\n",
    "#                 break\n",
    "    # Update the last point for the next segment's comparison\n",
    "        last_point = reconstructed_imp[seq, start + retain_length - 1]\n",
    "        last_second_point = reconstructed_imp[seq, start + retain_length - 2]\n",
    "        last_derivatives = last_point-last_second_point\n",
    "        start += retain_length\n",
    "    # break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3b72aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_impute_bdc",
   "language": "python",
   "name": "env_impute_bdc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
